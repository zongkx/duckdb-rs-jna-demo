use anyhow::{anyhow, Result};
use duckdb::Connection;
use log::{debug, error};
use rand::Rng;
use serde::Serialize;
use serde_json::to_string;
use std::fs::{self, File, OpenOptions};
use std::io::{self, Write};
use std::path::{Path, PathBuf};
use std::sync::{
    atomic::{AtomicUsize, Ordering},
    Arc, Mutex,
};
use std::time::Instant;

// DuckDBå†™å…¥å™¨çš„æ ¸å¿ƒç»“æ„ä½“
pub struct DuckDBCsvWriter {
    base_path: PathBuf,
    csv_file: PathBuf,
    pub row_counter: AtomicUsize,
    db_connection: Arc<Mutex<Connection>>,
    need_organize: bool,
    limit: usize,
    current_writer: Arc<Mutex<Option<io::BufWriter<File>>>>,
}

impl DuckDBCsvWriter {
    pub fn new(
        base_path: &str,
        db_connection: Arc<Mutex<Connection>>,
        csv_file_name: &str,
        need_organize: bool,
    ) -> Result<Self> {
        let base_path_buf = PathBuf::from(base_path);
        let csv_file_buf = base_path_buf.join(csv_file_name);

        let mut rng = rand::thread_rng();
        let limit = rng.gen_range(100..=200);

        // åˆå§‹åŒ–writer
        let writer = Self::create_new_writer(&csv_file_buf, false)?;

        Ok(Self {
            base_path: base_path_buf,
            csv_file: csv_file_buf,
            row_counter: AtomicUsize::new(0),
            db_connection,
            need_organize,
            limit,
            current_writer: Arc::new(Mutex::new(Some(writer))),
        })
    }

    pub fn write_rows<T: Serialize>(&self, list: &[T]) -> Result<()> {
        {
            let mut writer_guard = self.current_writer.lock().unwrap();
            let writer = writer_guard
                .as_mut()
                .ok_or_else(|| anyhow!("Writer is closed"))?;

            for item in list {
                let json_string = to_string(item)?;
                writeln!(writer, "{}", json_string)?;
            }
            writer.flush()?;
        } // ğŸ”‘ åœ¨è¿™é‡Œé‡Šæ”¾ writer_guard

        let new_count = self.row_counter.fetch_add(list.len(), Ordering::Relaxed) + list.len();

        if self.need_organize && new_count >= self.limit {
            self.organize()?; // è¿™é‡Œå°±ä¸ä¼šæ­»é”äº†
        }
        Ok(())
    }

    fn organize(&self) -> Result<()> {
        let start_time = Instant::now();
        let parquet_file_name = self.get_parquet_file_name()?;

        // æ­¥éª¤ä¸€ï¼šåœ¨æ‰§è¡Œè€—æ—¶æ“ä½œä¹‹å‰ï¼Œå…ˆå°† writer é‡Šæ”¾ï¼Œä»¥é¿å…é˜»å¡å…¶ä»–çº¿ç¨‹ã€‚
        let csv_file_path = {
            let mut writer_guard = self.current_writer.lock().unwrap();
            // å…³é—­å¹¶æ¸…ç©ºå½“å‰å†™å…¥å™¨ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å·²å†™å…¥æ–‡ä»¶
            writer_guard.take();
            self.csv_file.clone()
        }; // é”åœ¨è¿™é‡Œè¢«é‡Šæ”¾
        let x = &format!(
            "copy (select * from read_json ('{}')) to '{}' (format parquet, compression lz4_raw, PARQUET_VERSION v2)",
            self.csv_file.display(),
            parquet_file_name.display()
        );
        debug!("SQL: {}", x);
        let statement_result = self
            .db_connection
            .lock()
            .unwrap()
            .execute(x, &[] as &[&dyn duckdb::ToSql]);
        let success = statement_result.is_ok();

        if !success {
            error!("duckdb merge error: {:?}", statement_result.unwrap_err());
            fs::remove_file(&parquet_file_name).ok();
        }

        // æ­¥éª¤ä¸‰ï¼šé‡æ–°è·å– writer çš„é”ï¼Œå¹¶é‡ç½®çŠ¶æ€ã€‚
        let mut writer_guard = self.current_writer.lock().unwrap();
        self.row_counter.store(0, Ordering::Relaxed);
        *writer_guard = Some(Self::create_new_writer(&self.csv_file, success)?);

        debug!(
            "æ–‡ä»¶åˆå¹¶ä»»åŠ¡ç»“æŸ: {}, è€—æ—¶: {}ms",
            self.csv_file.display(),
            start_time.elapsed().as_millis()
        );

        Ok(())
    }

    fn create_new_writer(path: &Path, overwrite: bool) -> Result<io::BufWriter<File>> {
        fs::create_dir_all(path.parent().unwrap_or(Path::new("")))?;

        let file = if overwrite || !path.exists() {
            File::create(path)?
        } else {
            OpenOptions::new().append(true).create(true).open(path)?
        };

        Ok(io::BufWriter::new(file))
    }

    fn get_parquet_file_name(&self) -> Result<PathBuf> {
        let timestamp = chrono::Utc::now().timestamp_millis();
        Ok(self.base_path.join(format!("{}.parquet", timestamp)))
    }
}
